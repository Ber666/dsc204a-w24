---
title: Week 10
class: DSC204A
status: Active
---

Mar 11
: **1**{: .label} ML System - 2
  : [Slides](assets/slides/22_ml-system-2.pdf) &#8226; [Recording]() &#8226; [Scribe Notes](#)
: *Reading:*
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [PGPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)




Mar 13
: **2**{: .label} ML System - 3
  : [Slides](assets/slides/23_ml-system-3.pdf) &#8226; [Recording](#) &#8226; [Scribe Notes](#)
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [PGPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)




Mar 15
: **3**{: .label} ML System - 4
  : [Slides](assets/slides/24_ml-system-4.pdf) &#8226; [Recording]() &#8226; [Scribe Notes](#)
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [PGPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)

